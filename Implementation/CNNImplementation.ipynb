{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5f587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from summa import keywords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding, Conv1D, MaxPooling1D, concatenate, Attention, Lambda\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6fe53",
   "metadata": {},
   "source": [
    "## Develop CNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4efd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a990cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3029668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899b0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(length, vocab_size):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "\n",
    "    # Convolutional layer\n",
    "    conv = Conv1D(filters=32, kernel_size=2, activation='relu')(embedding)\n",
    "    drop = Dropout(0.2)(conv)\n",
    "    pool = MaxPooling1D(pool_size=3)(drop)\n",
    "    flat = Flatten()(pool)\n",
    "\n",
    "    # Attention layer\n",
    "    atten = Attention()([conv, conv])  # Use the Attention layer here\n",
    "\n",
    "    # Attention weights\n",
    "    atten_weights = Lambda(lambda x: K.mean(x, axis=1))(atten)\n",
    "    \n",
    "    # Merge attention weights with main output\n",
    "    merged = concatenate([flat, atten_weights])\n",
    "\n",
    "    # Dense layers\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    main_output = Dense(3, activation='softmax', name='main_output')(dense1)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=[main_output, atten_weights])\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss={'main_output': 'sparse_categorical_crossentropy'}, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='singlechannel_with_attention.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051b298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 3158\n",
      "Vocabulary size: 11931\n",
      "(13542, 3158)\n"
     ]
    }
   ],
   "source": [
    " # load training dataset\n",
    "trainLines, trainLabels = load_dataset('../Dataset/train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e23d0ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3158)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 3158, 100)    1193100     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 3157, 32)     6432        ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3157, 32)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 1052, 32)     0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 3157, 32)     0           ['conv1d[0][0]',                 \n",
      "                                                                  'conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 33664)        0           ['max_pooling1d[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 32)           0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 33696)        0           ['flatten[0][0]',                \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           336970      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 3)            33          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,536,535\n",
      "Trainable params: 1,536,535\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "424/424 [==============================] - 925s 2s/step - loss: 0.6460 - main_output_loss: 0.6460 - main_output_accuracy: 0.6969 - lambda_accuracy: 0.0016\n",
      "Epoch 2/15\n",
      "424/424 [==============================] - 983s 2s/step - loss: 0.1334 - main_output_loss: 0.1334 - main_output_accuracy: 0.9647 - lambda_accuracy: 0.0011\n",
      "Epoch 3/15\n",
      "424/424 [==============================] - 981s 2s/step - loss: 0.0650 - main_output_loss: 0.0650 - main_output_accuracy: 0.9851 - lambda_accuracy: 0.0030\n",
      "Epoch 4/15\n",
      "424/424 [==============================] - 954s 2s/step - loss: 0.0507 - main_output_loss: 0.0507 - main_output_accuracy: 0.9884 - lambda_accuracy: 0.0038\n",
      "Epoch 5/15\n",
      "424/424 [==============================] - 988s 2s/step - loss: 0.0500 - main_output_loss: 0.0500 - main_output_accuracy: 0.9889 - lambda_accuracy: 0.0032\n",
      "Epoch 6/15\n",
      "424/424 [==============================] - 979s 2s/step - loss: 0.0381 - main_output_loss: 0.0381 - main_output_accuracy: 0.9912 - lambda_accuracy: 0.0046\n",
      "Epoch 7/15\n",
      "424/424 [==============================] - 984s 2s/step - loss: 0.0357 - main_output_loss: 0.0357 - main_output_accuracy: 0.9912 - lambda_accuracy: 0.0097\n",
      "Epoch 8/15\n",
      "424/424 [==============================] - 987s 2s/step - loss: 0.0296 - main_output_loss: 0.0296 - main_output_accuracy: 0.9918 - lambda_accuracy: 0.0136\n",
      "Epoch 9/15\n",
      "424/424 [==============================] - 952s 2s/step - loss: 0.0390 - main_output_loss: 0.0390 - main_output_accuracy: 0.9894 - lambda_accuracy: 0.0267\n",
      "Epoch 10/15\n",
      "424/424 [==============================] - 939s 2s/step - loss: 0.0299 - main_output_loss: 0.0299 - main_output_accuracy: 0.9906 - lambda_accuracy: 0.0304\n",
      "Epoch 11/15\n",
      "424/424 [==============================] - 933s 2s/step - loss: 0.0295 - main_output_loss: 0.0295 - main_output_accuracy: 0.9912 - lambda_accuracy: 0.0504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='main_output_accuracy', patience=3)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(trainX, array(trainLabels), epochs=15, batch_size=32,\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# save the model\n",
    "model.save('Models/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "778122e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../Models/cnn.h5'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Save the model\n",
    "model.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88f57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
