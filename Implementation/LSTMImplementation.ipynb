{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f5f587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from pickle import dump, load\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Embedding, Conv1D, MaxPooling1D, concatenate\n",
    "from keras.layers import Attention, Lambda, Bidirectional\n",
    "from keras.layers import GRU, Dense, Dropout, Flatten, Input, Embedding, concatenate, Bidirectional, Attention , Lambda \n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from keras import backend as K\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f6fe53",
   "metadata": {},
   "source": [
    "## Develop LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad4efd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_dataset(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a990cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "309e9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "\treturn max([len(s.split()) for s in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3029668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "\t# integer encode\n",
    "\tencoded = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad encoded sequences\n",
    "\tpadded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "\treturn padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "899b0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(length, vocab_size):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(length,))\n",
    "    embedding = Embedding(vocab_size, 100)(inputs)\n",
    "\n",
    "    # LSTM layer\n",
    "    lstm1 = LSTM(100, return_sequences=True)(embedding)\n",
    "    drop = Dropout(0.3)(lstm1)\n",
    "    \n",
    "    # Attention layer\n",
    "    atten = Attention()([drop, drop])  \n",
    "    # Attention weights\n",
    "    atten_weights = Lambda(lambda x: K.mean(x, axis=1))(atten)\n",
    "    \n",
    "    # Flatten the LSTM output\n",
    "    flat = Flatten()(drop)\n",
    "\n",
    "    # Merge attention weights with main output\n",
    "    merged = concatenate([flat, atten_weights])\n",
    "\n",
    "    # Dense layers\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    dense2 = Dense(50, activation='relu')(dense1)  \n",
    "    main_output = Dense(3, activation='softmax', name='main_output')(dense2)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=[main_output, atten_weights])\n",
    "\n",
    "    # Compile\n",
    "    model.compile(loss={'main_output': 'sparse_categorical_crossentropy'}, optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    # Summarize\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='LSTMModel.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051b298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 3158\n",
      "Vocabulary size: 11931\n",
      "(13542, 3158)\n"
     ]
    }
   ],
   "source": [
    " # load training dataset\n",
    "trainLines, trainLabels = load_dataset('../Dataset/train.pkl')\n",
    "# create tokenizer\n",
    "tokenizer = create_tokenizer(trainLines)\n",
    "# calculate max document length\n",
    "length = max_length(trainLines)\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# encode data\n",
    "trainX = encode_text(tokenizer, trainLines, length)\n",
    "print(trainX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e23d0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 3158)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 3158, 100)    1193100     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 3158, 100)    80400       ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 3158, 100)    0           ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " attention (Attention)          (None, 3158, 100)    0           ['dropout[0][0]',                \n",
      "                                                                  'dropout[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 315800)       0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 100)          0           ['attention[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 315900)       0           ['flatten[0][0]',                \n",
      "                                                                  'lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           3159010     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 50)           550         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 3)            153         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,433,213\n",
      "Trainable params: 4,433,213\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/15\n",
      "424/424 [==============================] - 4752s 11s/step - loss: 0.6077 - main_output_loss: 0.6077 - main_output_accuracy: 0.7249 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 2/15\n",
      "424/424 [==============================] - 5067s 12s/step - loss: 0.1724 - main_output_loss: 0.1724 - main_output_accuracy: 0.9495 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 3/15\n",
      "424/424 [==============================] - 5134s 12s/step - loss: 0.1044 - main_output_loss: 0.1044 - main_output_accuracy: 0.9722 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 4/15\n",
      "424/424 [==============================] - 5113s 12s/step - loss: 0.0753 - main_output_loss: 0.0753 - main_output_accuracy: 0.9800 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 5/15\n",
      "424/424 [==============================] - 5263s 12s/step - loss: 0.0563 - main_output_loss: 0.0563 - main_output_accuracy: 0.9848 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 6/15\n",
      "424/424 [==============================] - 5201s 12s/step - loss: 0.0503 - main_output_loss: 0.0503 - main_output_accuracy: 0.9857 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 7/15\n",
      "424/424 [==============================] - 6469s 15s/step - loss: 0.0348 - main_output_loss: 0.0348 - main_output_accuracy: 0.9900 - lambda_accuracy: 2.9538e-04\n",
      "Epoch 8/15\n",
      "424/424 [==============================] - 6736s 16s/step - loss: 0.0440 - main_output_loss: 0.0440 - main_output_accuracy: 0.9860 - lambda_accuracy: 0.0049\n",
      "Epoch 9/15\n",
      "424/424 [==============================] - 6899s 16s/step - loss: 0.0293 - main_output_loss: 0.0293 - main_output_accuracy: 0.9910 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 10/15\n",
      "424/424 [==============================] - 6218s 15s/step - loss: 0.0270 - main_output_loss: 0.0270 - main_output_accuracy: 0.9905 - lambda_accuracy: 1.4769e-04\n",
      "Epoch 11/15\n",
      "424/424 [==============================] - 5316s 13s/step - loss: 0.0312 - main_output_loss: 0.0312 - main_output_accuracy: 0.9904 - lambda_accuracy: 0.0000e+00\n",
      "Epoch 12/15\n",
      "424/424 [==============================] - 5952s 14s/step - loss: 0.0464 - main_output_loss: 0.0464 - main_output_accuracy: 0.9853 - lambda_accuracy: 2.9538e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc3182d190>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='main_output_accuracy', patience=3)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model.fit(trainX, np.array(trainLabels), epochs=15, batch_size=32,\n",
    "          callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1078e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the file path\n",
    "file_path = '../Models/lstm.h5'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    # Delete the file\n",
    "    os.remove(file_path)\n",
    "\n",
    "# Save the model\n",
    "model.save(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
